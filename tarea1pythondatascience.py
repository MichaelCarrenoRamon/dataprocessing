# -*- coding: utf-8 -*-
"""Tarea1PythonDataScience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKikNx_NqTZk9YGgQ4ASDruGqVVWx3hL

# **Trabajo Final de Curso: Python - Data Science**

Nombre: Michael Enrique Carre√±o Ramon

## **Informaci√≥n de Dataset elegido**

El dataset **"Netflix Movies and TV Shows"** proporciona informaci√≥n de la plataforma de streaming Netflix. Incluye tanto Pel√≠culas como Shows de TV con car√°cteristicas principales que ser√°n utilizados para pr√°cticas de an√°lisis por su estructura.

Disponible en el siguiente link: https://www.kaggle.com/datasets/shivamb/netflix-shows?resource=download

## **Detalles de Dataset**

**show_id:** Identificador √∫nico

**type:** Tipo de contenido: Movie o TV Show

**title:** Nombre del contenido

**director:** Nombre del director o directora

**cast:** Lista de actores y actrices principales

**country:** Pa√≠s(es) de origen

**date_added:** A√±o en el que se agreg√≥ a la plataforma de Netflix

**release_year:** A√±o en que se realiz√≥ la producci√≥n del contenido

**rating:** Clasificaci√≥n por edad (TV-MA, PG, R, etc)

**duration:** Duraci√≥n de pel√≠cula (min) o cantidad de temporadas en serie

**listed_in:** Categor√≠a o g√©nero del contenido

**description:** Sipnosis del contenido

## **Objetivo del an√°lisis**

Este proyecto es para realizar un an√°lisis exploratorio de todo el dataset con el fin de:



*   Preparar los datos mediante t√©cnicas de limpieza y tratamiento de valores nulos.
*   Mejorar la calidad del dataset mediante la detecci√≥n y eliminaci√≥n de valores duplicados.
*   Generar visualizaciones usando herramientas como Matplotlib, Seaborn o Plotly, con etiquetas, leyendas y descripciones interpretativas que respalden los hallazgos.

# **1. Cargar el Dataset**

## Importar librer√≠as
"""

# Importar librerias

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Detalle de librerias**

- Pandas: Librer√≠a dise√±ada para la manipulaci√≥n y an√°lisis de datos estructurados, como tablas tipo hoja de c√°lculo o bases de datos, as√≠ como series temporales.

- NumPy (Numerical Python): Biblioteca fundamental para c√°lculos num√©ricos, que proporciona soporte para arreglos y matrices grandes y multidimensionales, junto con funciones matem√°ticas de alto rendimiento.

- Matplotlib: Librer√≠a de visualizaci√≥n en Python que permite crear gr√°ficos est√°ticos, interactivos y animados de forma flexible.

- Seaborn: Extensi√≥n de Matplotlib enfocada en visualizaci√≥n estad√≠stica. Ofrece gr√°ficos m√°s estilizados y facilita el an√°lisis exploratorio de datos.

## Opci√≥n 1: Desde Google Drive (requiere conexi√≥n a internet)
"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Phawai + Tarecda/netflix_titles.csv')

"""Nota: Este m√©todo debe ejecutarse √∫nicamente si el archivo se encuentra previamente alojado en tu Google Drive.

## Opci√≥n 2: Desde el ordenador local (modo manual)
"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('netflix_titles.csv')

"""Nota: Este m√©todo es √∫til si no deseas utilizar Google Drive o si el entorno de ejecuci√≥n no requiere conexi√≥n a internet previa. Aplica en casos donde el archivo se carga directamente desde tu dispositivo.

## **Conclusi√≥n**
En esta etapa hemos explorado dos m√©todos para cargar el dataset "Netflix Movies and TV Shows".

Por un lado, la carga desde Google Drive, ideal para entornos colaborativos como Google Colab, requiere conexi√≥n a internet activa y acceso al archivo previamente alojado en la nube.

Por otro lado, la carga local desde el ordenador permite trabajar sin conexi√≥n, ya sea en notebooks locales o entornos offline como Visual Studio Code. Esta alternativa resulta √∫til cuando no se dispone de conectividad o se busca ejecutar el an√°lisis de forma aut√≥noma.

# **2. Carga y procesamiento de datos**

El procesamiento de datos es la conversi√≥n de datos sin procesar en informaci√≥n utilizable  a trav√©s de pasos estructurados como la recopilaci√≥n, la preparaci√≥n, el an√°lisis y el almacenamiento de datos. Las organizaciones pueden obtener conocimiento pr√°ctico y fundamentar la toma de decisiones procesando los datos con eficacia.

## 2.1. Exploraci√≥n de Dataframe
"""

# Visualizar las primeras filas del dataset

df.head()

"""Nota: El .head() se lo utiliza para tener una vista r√°pida de los primeros datos, por lo general las primeras 5 filas"""

# Visualizar las √∫ltimas filas del dataset

df.tail()

"""Nota: Muestra los registros finales del DataFrame"""

# Visualizar la informaci√≥n del dataset

df.info()

"""Nota: Resume la estructura del DataFrame es ideal para chequear tipos de datos y nulos"""

# Detectar valores at√≠picos o rangos inesperados

df.describe()

"""Nota: Muestra las estad√≠sticas descriptivas de columnas num√©ricas, se la utiliza principalmente para an√°lizar de forma r√°pida la distribuci√≥n y rango"""

df.shape

"""Esta linea devuelve el tama√±o del DataFrame como una tupla (filas, columnas)

**Nota Importante** antes de la evaluaci√≥n

El dataset indica que se importaron correctamente:

**8807 filas** ‚Üí cantidad de registros en el dataset

**12 columnas** ‚Üí cantidad de variables o caracter√≠sticas que existen por registro

## 2.2. Evaluaci√≥n Inicial del Dataset
"""

# Identificar variables num√©ricas y categ√≥ricas

numericas = df.select_dtypes(include=[np.number]).columns.tolist()
categoricas = df.select_dtypes(include=[object]).columns.tolist()

# Mostrar las variables num√©ricas

print("Lista de Variables Num√©ricas:")
for v in numericas:
    print("  -", v)

# Mostrar las variables categ√≥ricas

print("Lista de Variables Categ√≥ricas:")
for v in categoricas:
    print("  -", v)

"""Lo primero que realic√© fue separar las columnas del DataFrame en dos tipos: num√©ricas (como int, float) y categ√≥ricas (como str o object) para poder identificar con que valores num√©ricos y que valores de texto cuento dentro de mi DataFrame.

Me sirve para determinar qu√© tipo de an√°lisis aplicar

**Variables num√©ricas** ‚Üí puedo aplicar estad√≠stica descriptiva, histograma, correlaciones.

**Variables categ√≥ricas** ‚Üí puedo realizar conteo de frecuencias, gr√°ficos de barras, agrupaciones.

Por lo que puedo notar e interpretar que solo cuento con un valor num√©rico (release_year) lo que indica que la mayor√≠a de las columnas son de tipo categ√≥rico (texto).
"""

# Verificar en todo el dataset las filas duplicadas

filas_duplicadas = df.duplicated().sum()
print(f'Filas duplicadas: {filas_duplicadas}')

"""Luego realic√© un conteo de cu√°ntas filas exactas se repiten en el dataset.
Como podemos visualizar no cuento con ninguna fila duplicada lo que es una buena se√±al ya que no hay registros id√©nticos que distorsionen la distribuci√≥n.
"""

# Verificar si existen valores faltantes en el dataset

valores_faltantes = df.isnull().sum()

# Mostrar los valores faltantes por columna

print(f'Valores faltantes por columna:\n{valores_faltantes}')

"""Realic√© una verificaci√≥n de valores faltantes y podemos determinar que:

*   director tiene 2,634 valores faltantes lo cual es una cifra significativa.
*   cast y country tambien tiene valores faltantes, menos que director, pero sigue siendo una cifra que debemos interpretar de que forma solucionar la falta de valores dependiendo de como queramos continuar con nuestro an√°lisis.
*   date_added tambien tiene 10 valores faltantes, casi nada a comparaci√≥n de los otros dos casos.
*   rating y duration tambien tiene 4 y 3 valores faltantes respectivamente, los que menos valores faltante tienes hasta el an√°lisis de este dataset.

## **Conclusi√≥n**
Realizamos una inspecci√≥n general del dataset para validar su estructura y variables antes de iniciar con la limpieza de datos para mejorar la calidad del mismo.
Identificamos que:



*   El dataset contiene 8807 registros y 12 columnas
*   La mayor√≠a de las variables son categ√≥ricas, con release_year como la √∫nica num√©rica.
*   No existen filas duplicadas
*   Existen valores faltantes en diferentes columnas, unas con mayor impacto sobre el dataset sobre otras, lo que nos plantea a tomar decisiones de como manejar esos valores faltantes

Esta evaluaci√≥n general nos permite tener un mejor enfoque con que dataset estamos trabajando y avanzar con claridad para la etapas de limpieza, transformaci√≥n, normalizaci√≥n y visualizaci√≥n con una base s√≥lida.

# **3. Limpieza de Datos**

La limpieza de datos, tambi√©n llamada depuraci√≥n de datos, es el proceso de identificar, corregir y eliminar errores, inconsistencias e informaci√≥n irrelevante de un conjunto de datos para mejorar su calidad y precisi√≥n.

## 3.1. Eliminaci√≥n de Columnas
"""

# Eliminar columnas con mayor cantidad de valores n√∫los

df.drop(['director', 'cast', 'country'], axis=1, inplace=True)

print(df.columns)

"""Decisi√≥n T√©cnica: Se eliminaron las columnas director, cast, country debido a sus grandes cantidades de valores nulos. La alternativa de eliminar filas no es recomendable por la significativa p√©rdida de datos lo cual no ser√≠a eficiente para realizar un an√°lsis.

**Motivos de eliminaci√≥n de columnas:**

*   Reduce ruido en el an√°lisis
*   Mejora rendimiento en operaciones y visualizaciones
*   Simplifica la estructura del dataframe

Resultado: El dataframe ahora contiene 9 columnas




"""

# Volvemos a verificar el los valores nulos

df.isnull().sum()

"""## 3.2. Eliminaci√≥n de Filas"""

# Eliminar filas con valores nulos

df.dropna(inplace=True)

# Verificamos el n√∫mero de filas luego de la eliminaci√≥n

len(df)

df.isnull().sum()

"""Nota: Acab√© de realizar la eliminaci√≥n de la columnas con valores nulos por lo cual podemos ver que ya no existen valores vac√≠os dentro de nuestro dataset"""

# Verificamos la nueva dimensi√≥n de nuestro dataset

df.shape

"""Nota: Podemos ver que nuestro dataset ahora tiene 8790 filas y 9 columnas luego de limpiar valores nulos"""

# Volvemos a visualizar como se ve nuestro dataset

df.head()

"""Nota: Podemos observar que ya no existen valores nulos.

Paso opcional: Descargar la nueva data procesada
"""

from google.colab import files

# Guardar el DataFrame como un archivo CSV

df.to_csv('netflix_titles_preprocessed.csv', index=False)

# Descargar el archivo CSV

files.download('netflix_titles_preprocessed.csv')

"""## 3.3. Normalizaci√≥n de datos

La normalizaci√≥n de datos es un proceso para organizar los datos en una base de datos de manera que se reduzca la redundancia y se mejore la integridad.

### 3.3.1. Columna duration
"""

# Extrae solo el n√∫mero

df['duration_num'] = df['duration'].str.extract('(\d+)').astype(float)

# Extrae solo el tipo de duraci√≥n sin crear columnas adicionales

df['duration_type'] = df['duration'].apply(lambda x: 'min' if isinstance(x, str) and 'min' in x.lower() else 'season')

# Eliminaci√≥n de columna original

df.drop(['duration'], axis=1, inplace=True)

print(df.columns.tolist())

df.head()

"""Decisi√≥n T√©cnica: Se descompuso la variable mixta duration, originalmente en formato string, en dos componentes expl√≠citos:

* duration_num: valor num√©rico de duraci√≥n extra√≠do mediante expresi√≥n regular.

* duration_type: unidad de medida derivada del contenido textual (min o season).

**Motivos de Normalizaci√≥n**

* Facilitar el tratamiento por separado en modelos predictivos o filtrado por unidad.

* Establecer una estructura uniforme que evita ambig√ºedades y mejora la legibilidad en el pipeline.

### 3.3.2. Columna rating
"""

# Verificamos cuantos datos existen en la columna rating

print(df['rating'].unique())

"""Nota: Este paso es crucial para verificar cuantos datos existen en la columan rating ya que si no colocamos todos podemos generar valores nulos"""

# Definir el diccionario de mapeo

rating_map = {
    'PG-13': 0, 'TV-MA': 1, 'PG': 2, 'TV-14': 3,
    'TV-PG': 4, 'TV-Y': 5, 'TV-Y7': 6, 'R': 7,
    'TV-G': 8, 'G': 9, 'NC-17': 10, 'NR': 11,
    'TV-Y7-FV': 12, 'UR': 13
}

# Aplicar el mapeo

df['rating_encoded'] = df['rating'].map(rating_map)

# Eliminar la columna original

df.drop(['rating'], axis=1, inplace=True)

"""Nota: Convierto la cadena de texto en variables num√©ricas

* 'PG-13': 0
* 'TV-MA': 1
* 'PG': 2
* 'TV-14': 3
* 'TV-PG': 4
* 'TV-Y': 5
* 'TV-Y7': 6
* 'R': 7
* 'TV-G': 8
* 'G': 9
* 'NC-17': 10
* 'NR': 11
* 'TV-Y7-FV': 12
* 'UR': 13
"""

print(df.columns)

df.tail()

"""Decisi√≥n T√©cnica: Se verific√≥ previamente la variedad de categor√≠as presentes esto permiti√≥ construir un diccionario de codificaci√≥n completo, evitando la generaci√≥n de valores nulos durante la transformaci√≥n. Una vez codificados los datos, se elimin√≥ la columna original para mantener un esquema limpio y evitar redundancias.

**Motivos de Normalizaci√≥n**

* Visualizaciones m√°s precisas al tener valores para visualizarlos de forma cualitativas.
* An√°lisis categ√≥rico ordenado.

### 3.3.3. Columna listen_in
"""

# Creamos columnas a ra√≠z de las cadenas de textos existentes en la columna 'listen_in'

df_genres = df['listed_in'].str.get_dummies(sep=', ')
df = pd.concat([df, df_genres], axis=1)

df.head()

# Verificamos la creaci√≥n de las columnas nuevas a partir de listen_in

print(df.columns)

# Verificamos que luego de la normalizaci√≥n no existan valores nulos

df.isnull().sum()

"""Decisi√≥n t√©cnica: La estructura previa no es directamente recomendable para analizarla de manera cuantitativo ni para segmentaci√≥n por categor√≠a. Por ello, se aplic√≥ una t√©cnica de normalizaci√≥n basada en codificaci√≥n binaria con get_dummies(), que permite convertir los g√©neros en columnas individuales con valores booleanos (1 si el g√©nero est√° presente en el registro, 0 si no).

**Motivos para la normalizaci√≥n**

* Cada g√©nero se representa como variable propia, evitando ambig√ºedad.
* Nos va a permitir filtrar, agrupar o visualizar t√≠tulos por g√©nero espec√≠fico.

Paso opcional: Descargar la nueva data procesada
"""

from google.colab import files

# Guardar el DataFrame como un archivo CSV

df.to_csv('netflix_titles_preprocessed.csv', index=False)

# Descargar el archivo CSV

files.download('netflix_titles_preprocessed.csv')

"""## **Conlusi√≥n**

Durante esta fase se implementaron transformaciones clave sobre variables mixtas y categ√≥ricas del dataset, priorizando la estructuraci√≥n, trazabilidad y compatibilidad anal√≠tica del contenido.

Aplicamos la siguietes:
* Transformaci√≥n de la columna duration
* Codificaci√≥n de la variable rating
* Normalizaci√≥n de la columna listed_in

Estas transformaciones aseguran una visualizaci√≥n clara y precisa del dataset, habilitando el descubrimiento de patrones y tendencias de forma accesible para usuarios t√©cnicos y no t√©cnicos. Se fortaleci√≥ adem√°s la preparaci√≥n del DataFrame para futuras etapas como an√°lisis exploratorio, modelado o segmentaci√≥n.

# **4. Visualizaci√≥n**

La visualizaci√≥n de datos es la representaci√≥n gr√°fica de informaci√≥n y datos. Al utilizar elementos visuales como cuadros, gr√°ficos y mapas, las herramientas de visualizaci√≥n de datos proporcionan una manera accesible de ver y comprender tendencias, valores at√≠picos y patrones en los datos.

## 4.1. Heatmap
"""

# Seleccionar columnas binarias de g√©neros (a partir del √≠ndice 10 como antes)
generos_binarios = df.columns[10:]

# Lista para construir el nuevo DataFrame
heatmap_lista = []

# Recorrer cada fila del DataFrame original
for _, fila in df.iterrows():
    for genero in generos_binarios:
        if fila[genero] == 1:  # Si ese t√≠tulo pertenece a ese g√©nero
            heatmap_lista.append({
                'G√©nero': genero,
                'Rating': fila['rating_encoded']  # O usa 'rating' si prefieres el original
            })

# Crear DataFrame para el heatmap
heatmap_df = pd.DataFrame(heatmap_lista)

# Agrupar y contar
heatmap_data = heatmap_df.groupby(['G√©nero', 'Rating']).size().unstack(fill_value=0)

# Graficar
plt.figure(figsize=(16, 20))
sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlOrBr')
plt.title('Cantidad de T√≠tulos por G√©nero y Clasificaci√≥n (Rating)')
plt.xlabel('Clasificaci√≥n')
plt.ylabel('G√©nero')
plt.tight_layout()
plt.show()

"""**Interpretaci√≥n**

El eje y (vertical) est√° compuesto por los G√©neros que tenemos en nuestro dataset

El eje x (horizontal) contiene los valores de las clasificaciones de contenido

Cada celda representa el n√∫mero de t√≠tulos que pertenecen a un g√©nero espec√≠fico y poseen una clasificaci√≥n determinada.

La intensidad de color ( en este caso utilizamos un mapa magma) muestra la frecuencia: colores m√°s claros indican mayor presencia; colores oscuros, menor frecuencia o ausencia.

**Decisi√≥n t√©cnica**

Utilic√© un heatmap porque permite visualizar simult√°neamente 40+ g√©neros y 14 clasificaciones sin perder claridad. Cosa que no ser√≠a muy bien visto en un diagrama de barras, o un gr√°fico de pastel.

**Conclusiones**

* Se observa que g√©neros como ‚ÄúComedies‚Äù, ‚ÄúRomantic TV Shows‚Äù y ‚ÄúTV Dramas‚Äù se distribuyen en clasificaciones medias (PG, TV-PG, TV-14), reflejando una intenci√≥n editorial de alcanzar p√∫blicos amplios.

* Por otro lado, g√©neros como ‚ÄúDocumentaries‚Äù y ‚ÄúCrime TV Shows‚Äù presentan una mayor concentraci√≥n en clasificaciones TV-MA o R, indicando tem√°ticas m√°s adultas o complejas.

* G√©neros infantiles como ‚ÄúKids‚Äô TV‚Äù se agrupan fuertemente en TV-Y y TV-Y7, lo que valida la normalizaci√≥n y codificaci√≥n aplicada previamente. Este patr√≥n de distribuci√≥n por clasificaci√≥n permite segmentar claramente el contenido por rango de edad.

## 4.2. Gr√°fico de Burbujas
"""

# Seleccionar columnas binarias de g√©nero
generos_binarios = df.columns[10:]  # A partir de la columna 10 en adelante

# Verificaci√≥n: existen columnas de duraci√≥n
if 'duration_num' not in df.columns:
    raise ValueError("La columna 'duration_num' no existe en el dataset.")

# Crear lista para agrupar info
bubble_data = []

for genero in generos_binarios:
    if genero in df.columns:
        subset = df[df[genero] == 1]
        if not subset.empty:
            promedio_duracion = subset['duration_num'].mean()
            conteo = subset.shape[0]
            bubble_data.append({
                'Genero': genero,
                'Duracion_Promedio': promedio_duracion,
                'Cantidad': conteo
            })

# Convertir a DataFrame
bubble_df = pd.DataFrame(bubble_data)

# Verifica que las columnas existan correctamente
print("Columnas en bubble_df:", bubble_df.columns)
print(bubble_df.head())

# Graficar
plt.figure(figsize=(14, 10))
sns.scatterplot(
    data=bubble_df,
    x='Cantidad',
    y='Genero',
    size='Duracion_Promedio',
    hue='Duracion_Promedio',
    sizes=(100, 1500),
    palette='coolwarm',
    alpha=0.7,
    legend='brief'
)
plt.title('Duraci√≥n Promedio vs Cantidad de T√≠tulos por G√©nero (Burbuja)')
plt.xlabel('Cantidad de T√≠tulos')
plt.ylabel('G√©nero')
plt.tight_layout()
plt.show()

"""**Interpretaci√≥n**

El eje y (vertical) es el n√∫mero de los t√≠tulos por g√©nero

El eje x (horizontal) contiene los g√©neros normalizado (columna binaria con presencia)

El tama√±o de la burbuja muestra la duraci√≥n promedio en minutos (Duracion_Promedio)

Cada burbuja representa una categor√≠a de g√©nero con su volumen en el cat√°logo y su tendencia de duraci√≥n, y el color de la burbuja tambi√©n representa duraci√≥n promedio, permitiendo doble codificaci√≥n visual

**Decisi√≥n t√©cnica**

Se utiliz√≥ el gr√°fico de burbuja por su visi√≥n multidimensional condensada lo que muestran 3 variables a la vez sin saturar la lectura. Permite una visualizaci√≥n clara y entender c√≥mo la duraci√≥n se relaciona con el volumen por g√©nero.

**Conclusiones**

* G√©neros con burbujas grandes y claras (como ‚ÄúDocumentaries‚Äù o ‚ÄúCult Movies‚Äù) tienden a tener t√≠tulos m√°s extensos.

* G√©neros con muchas obras pero burbuja peque√±a y oscura (como ‚ÄúReality TV‚Äù o ‚ÄúTeen TV Shows‚Äù) indican una gran cantidad de t√≠tulos, pero con duraci√≥n corta.

* Es f√°cil detectar g√©neros con alta producci√≥n pero baja duraci√≥n promedio, lo que puede sugerir formatos epis√≥dicos, dirigidos a consumo breve o p√∫blico juvenil.

## 4.3. Gr√°fico de √°reas apiladas
"""

# Agrupar datos por a√±o y tipo
year_type_counts = df.groupby(['release_year', 'type']).size().reset_index(name='count')

# Pivotear la tabla para que cada tipo sea una columna
pivot_df = year_type_counts.pivot(index='release_year', columns='type', values='count').fillna(0)

# Ordenar por a√±o
pivot_df = pivot_df.sort_index()

# Configurar estilo
plt.figure(figsize=(14, 8))
pivot_df.plot(kind='area', stacked=True, colormap='Set2', figsize=(14, 8), alpha=0.8)

# T√≠tulos y etiquetas
plt.title('Distribuci√≥n de Movies y TV Shows por A√±o en Netflix', fontsize=16)
plt.xlabel('A√±o de Lanzamiento', fontsize=12)
plt.ylabel('Cantidad de T√≠tulos', fontsize=12)
plt.legend(title='Tipo de Contenido', fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""**Interpretaci√≥n**

El eje y (vertical) representa el release_year, desde antes de 1940 hasta 2020.

El eje x (horizontal) muestra el n√∫mero de t√≠tulos lanzados cada a√±o.

Se diferencian dos categor√≠as:

üü© Movies (color verde)

‚¨ú TV Shows (color gris)

Las √°reas est√°n apiladas (‚Äústacked‚Äù), mostrando el total acumulado a√±o por a√±o.

**Decisi√≥n t√©cnica**

Se utiliz√≥ el gr√°fico de √°reas apiladas para mostrar tendencias temporales acumuladas, y c√≥mo una categor√≠a influye sobre el total. Nos permite ver solapamiento y crecimiento relativo entre tipos de contenido a√±o tras a√±o.

**Conclusiones**

* Desde 2000 en adelante se observa una aceleraci√≥n exponencial en la cantidad de lanzamientos, sobre todo a partir de 2015, alineada con el crecimiento agresivo de Netflix como productora y distribuidora.

* Las peliculas (Movies) dominan el volumen hist√≥rico, pero los TV Shows incrementan significativamente en a√±os recientes, indicando una estrategia editorial m√°s enfocada en series para retenci√≥n de usuarios.

* Hay un comportamiento c√≠clico o picos de producci√≥n que podr√≠an asociarse con eventos globales o cambios en la industria del entretenimiento.

## **Conclusi√≥n**

Durante esta etapa se implementaron m√∫ltiples artefactos visuales para representar caracter√≠sticas clave del cat√°logo de Netflix. A trav√©s de visualizaciones avanzadas como heatmaps, gr√°ficos de burbujas y √°reas acumuladas.

La importancia de aplicar visualizaciones:

* Traducen datos en conocimiento narrativo
* Facilitan la toma de decisiones
* Mejoran la comunicaci√≥n t√©cnica y ejecutiva

# **5. Referencias**

[1] https://www.ibm.com/es-es/think/topics/data-processing

[2] https://www.tableau.com/learn/articles/what-is-data-cleaning#:~:text=limpieza%20de%20datos-,%C2%BFQu%C3%A9%20es%20la%20limpieza%20de%20datos?,realiza%20correctamente%20en%20todo%20momento.

[3] https://www.tableau.com/es-mx/learn/articles/data-visualization#:~:text=La%20visualizaci%C3%B3n%20de%20datos%20es,y%20patrones%20en%20los%20datos.
"""